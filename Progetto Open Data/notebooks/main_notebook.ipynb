{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4893ee6",
   "metadata": {},
   "source": [
    "# <center> **Tesina di Open Data Management 2022-2023**\n",
    "\n",
    "<br>\n",
    "<div align=\"center\">\n",
    "    <img src=\"op_wallpaper.jpg\" width=\"1200\" height=\"auto\">\n",
    "\n",
    "### <center> _Andrea Spinelli, Raffaele Terracino, Marco Valenti_\n",
    "##### <center> 23 Giugno \n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ed5dffb",
   "metadata": {},
   "source": [
    "# __Indice__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f4687743",
   "metadata": {},
   "source": [
    "###  [`1. Traccia`](#)\n",
    "### [`2. Selezione dei dataset`](#)\n",
    "##### &emsp;&emsp; [`2.1 Raccolta`](#)\n",
    "##### &emsp;&emsp; [`2.2 Licenze`](#)\n",
    "### [`3. Elaborazione dei dataset`](#)\n",
    "##### &emsp;&emsp; [`3.1 Pulizia`](#)\n",
    "##### &emsp;&emsp; [`3.2 Pivoting`](#)\n",
    "##### &emsp;&emsp; [`3.3 Arricchimento`](#)\n",
    "### [`4. Trasformazione dei dataset a 5 stelle`](#)\n",
    "##### &emsp;&emsp; [`4.1 Ontologia`](#)\n",
    "##### &emsp;&emsp; [`4.2 Creazione grafo RDF`](#)\n",
    "##### &emsp;&emsp; [`4.3 Interlinking del grafo RDF`](#)\n",
    "### [`5. Data visualization`](#)\n",
    "### [`6. Creazione di un'applicazione`](#)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a21597c8",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# __1 Traccia__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb5b2ab5",
   "metadata": {},
   "source": [
    "<u> Utilizzando il linguaggio Python, per lo sviluppo del progetto si devono innanzitutto rispettare i seguenti passi: </u>\n",
    "\n",
    "- _Selezione dati;_\n",
    "- _Elaborazione dati (data cleaning, definizione struttura omogenea);_\n",
    "- _Open Linked Data (creazione di uno strato semantico, ontologie, interlinking)._\n",
    " \n",
    "<u> Dopodich√© si pu√≤ passare, opzionalmente, allo sviluppo del servizio tramite: </u> \n",
    "- _Creazione di un bot telegram._\n",
    "\n",
    "<u> Siano i seguenti dataset (forniti in allegato): </u>\n",
    "\n",
    "__farmacie.csv,__\n",
    "\n",
    "__parafarmacie.csv,__\n",
    "\n",
    "__strutture_sanitarie_pubbliche.csv,__\n",
    "\n",
    "__strutture_sanitarie_private.csv.__\n",
    "\n",
    "<u> Si vuole realizzare un servizio in ambito ‚ÄúSalute‚Äù che fornisca le posizioni e le disponibilit√† delle strutture mediche nella regione siciliana. </u>\n",
    "\n",
    "Si suppone che un utente del servizio abbia bisogno di dover andare a fare una visita dermatologica (struttura privata), oppure una visita ad un consultorio (struttura pubblica), tuttavia non conosce la posizione della struttura; quindi, pu√≤ interrogare il servizio affinch√© possa trovare in un raggio di ùë• metri, data la sua posizione, un elenco (messaggio testuale, posizioni, ‚Ä¶) di strutture specializzate nel campo desiderato. \n",
    "\n",
    "√à possibile inoltre che l‚Äôutente, dopo una visita, abbia la prescrizione di alcuni farmaci; pertanto, avr√† bisogno della locazione delle farmacie, o parafarmacie, pi√π vicine. Il servizio di tale richiesta funziona allo stesso modo di quello precedente, inoltre si pu√≤ fornire anche il recapito telefonico per effettuare la prenotazione dei farmaci. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0902fd99",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# __2 Selezione dei dataset__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6624065d",
   "metadata": {},
   "source": [
    "\n",
    "### __2.1 Raccolta__\n",
    "\n",
    "I dataset utilizzati per la realizzazione della base di conoscenza riguardano Farmacie, Parafarmacie, Strutture sanitarie Pubbliche, Strutture Sanitarie Private e la Popolazione residente in Sicilia nell'anno 2023. L'obiettivo della base di conoscenza √® racchiudere tutti gli esercizi sanitari presenti in Sicilia.\n",
    "\n",
    "L'ultimo dataset, in particolare, risulta fondamentale per raccontare una storia basandosi sugli altri dataset.\n",
    "\n",
    "Seguono i link da cui sono stati reperiti i dataset\n",
    "\n",
    "`Farmacie:` <br>\n",
    "_https://www.dati.salute.gov.it/dati/dettaglioDataset.jsp?menu=dati&idPag=5_\n",
    "\n",
    "`Parafarmacie:` <br>\n",
    "_https://www.dati.salute.gov.it/dati/dettaglioDataset.jsp?menu=dati&idPag=7_\n",
    "\n",
    "`Strutture Sanitarie Pubbliche:` <br>\n",
    "_https://dati.regione.sicilia.it/catalogo/da5a0f1f-82b4-472f-b3dd-458295983a97_\n",
    "\n",
    "`Strutture Sanitarie Private:` <br>\n",
    "_http://pti.regione.sicilia.it/portal/page/portal/PIR_PORTALE/PIR_LaStrutturaRegionale/PIR_AssessoratoSalute/PIR_DipPianificazioneStrategica/PIR_Infoedocumenti/PIR_8713479.360776903/PIR_Strutturesanitarieprivateaccreditate_\n",
    "\n",
    "`Popolazione Sicilia, 1 Gen 2023:` <br>\n",
    "_http://dati.istat.it/Index.aspx?QueryId=19101_\n",
    "\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed748f4b",
   "metadata": {},
   "source": [
    "### __2.2 Licenze__\n",
    "\n",
    "`Farmacie:` Italian Open Data Licence v2.0\n",
    "\n",
    "`Parafarmacie:` Italian Open Data Licence v2.0\n",
    "\n",
    "`Strutture Sanitarie Pubbliche:` Creative Commons BY, versione 4.0\n",
    "\n",
    "`Strutture Sanitarie Private:` licenza specificata nel decreto legislativo n. 33/2013, che specifica che \"documenti, le informazioni e i dati oggetto di pubblicazione obbligatoria ai sensi della normativa vigente, resi disponibili anche a seguito dell'accesso civico di cui all'articolo 5, sono pubblicati in formato di tipo aperto ai sensi dell'articolo 68 del Codice dell'amministrazione digitale, di cui al decreto legislativo 7 marzo 2005, n. 82, e sono riutilizzabili ai sensi del decreto legislativo 24 gennaio 2006, n. 36, del decreto legislativo 7 marzo 2005, n. 82, e del decreto legislativo 30 giugno 2003, n. 196, senza ulteriori restrizioni diverse dall'obbligo di citare la fonte e di rispettarne l'integrit√†\"\n",
    "\n",
    "`Popolazione Sicilia, 1 Gen 2023:` Creative Commons BY, versione 3.0 (specificato al link https://www.istat.it/it/note-legali)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab935cc9",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# __3 Elaborazione dei dataset__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "19c195a9",
   "metadata": {},
   "source": [
    "### __3.1 Pulizia__\n",
    "\n",
    "Si √® notato innanzitutto che i dataset puri non erano tutti adatti allo stesso modo, infatti, si sono riscontrate due problematiche:\n",
    "1. encoding dei dataset differenti, a causa delle loro diversi origini;\n",
    "2. errori vari dovuti a `;` o `,` non correttamente inseriti.\n",
    "\n",
    "La problematica 1 si √® riscontrata a causa degli errori di lettura dei file `CSV` da parte del metodo read_csv() di Pandas, inoltre se ne √® veriificata la veridicit√† tramite il comando `file` di Unix<br>\n",
    "Per ovviare a tale problema si √® deciso prima di tutto come encoding pi√π adatto ai dataset scelti l'`UTF-8`, dopodich√© si √® passati alla conversione dell'encoding dei file tramite un convertitore online che, in quanto i file `CSV` sono alla base dei testi, non ha generato problemi problemi di conversione nei dataset.\n",
    "\n",
    "La problematica 2 si √® riscontrata tramite lettura e osservazioni preventive all'uso dei dataset, i quali quest'ultimi presentavano errori di compiliazioni facilmente individuabili, appunto, si √® fatto semplice uso dello strumento `Find and Replace`.\n",
    "\n",
    "I dataset relativi a questa prima elaborazione sono stati raccolti in: `../datasets/cleaned/`. <br>\n",
    "Gli strumenti adoperati per la pulizia dei dataset includono `frictionless`, `Open Refine`, l'estensione `Edit csv` di `VS Code` e `Microsoft Excel`\n",
    "\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3897fad4",
   "metadata": {},
   "source": [
    "### __3.2 Pivoting__\n",
    "\n",
    "Dopo la pulizia, proseguendo con l'elaborazione dei dataset, sono seguite altre due osservazioni:\n",
    "1. Alcune **righe** dei dataset presentano dati di regioni aggiuntive oltre la Sicilia (a cui √® stato circoscritto il progetto);\n",
    "2. Siccome i dataset sono circoscritti ad una regione, alcune **colonne** ripetevano un medesimo dato per tutti i dataset.\n",
    "\n",
    "Entrambe le osservazioni sono state ovviate sempre in fase di pre-processing mediante lo strumento `Excel`. <br>\n",
    "La 1. selezionando le righe cui presentavano il campo \"Regione = Sicilia\" e rimuovendo le altre; la 2., similmente, eliminando direttamente i campi superflui pi√π quelli non adatti allo scopo del progetto.\n",
    "\n",
    "Questa seconda elaborazione dei dataset √® stata riportata in: `../datasets/pivoted/`\n",
    "\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "977d6e85",
   "metadata": {},
   "source": [
    "### __3.3 Arricchimento__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ea9d3123",
   "metadata": {},
   "source": [
    "In conclusione della fase di elaborazione, successivo al pivoting, si √® introdotta una fase di arricchimento dei dataset.\n",
    "\n",
    "Tale scelta √® stata effettuata dopo varie osservazioni preventive per le fasi successive dei dataset, i motivi principi sono stati: <br>\n",
    "l'agevolazione della trasformazione dei dataset a 4 stelle nel formato RDF, <br>\n",
    "l'agevolazione della trasformazione dei dataset a 5 stelle per l'interlinking."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7982bf45",
   "metadata": {},
   "source": [
    "///some comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15fbd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "comuni = pd.read_csv(\"../datasets/pivoted/popolazione_sicilia.csv\", sep=',', skiprows=[1])\n",
    "farmacie = pd.read_csv('../datasets/pivoted/farmacie.csv', sep=';')\n",
    "\n",
    "comuni['Territorio'] = comuni['Territorio'].apply(lambda x : x.title().replace('√†', 'a\\'').replace('√®', 'e\\'').replace('√¨', 'i\\'').replace('√≤', 'o\\'').replace('√π', 'u\\''))\n",
    "farmacie['DESCRIZIONECOMUNE'] = farmacie['DESCRIZIONECOMUNE'].apply(lambda x : x.title())\n",
    "joined = comuni.merge(farmacie, left_on='Territorio', right_on='DESCRIZIONECOMUNE')\n",
    "\n",
    "df2=joined.drop_duplicates(subset=['Territorio'], keep='first')\n",
    "\n",
    "df3=df2.rename(columns={\"Territorio\": \"Comune\", \"Value\": \"PopolazioneTotale\", \"DESCRIZIONEPROVINCIA\" : \"Provincia\", \"SIGLAPROVINCIA\":\"SiglaProvincia\"})\n",
    "df3.to_csv('comuni_sicilia.csv', columns=['Comune', 'PopolazioneTotale', 'Provincia', 'SiglaProvincia'], index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "89245350",
   "metadata": {},
   "source": [
    "///some comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e305354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_private = pd.read_csv(\"../datasets/pivoted/strutture_sanitarie_private.csv\", delimiter=\";\")\n",
    "df_pubbliche = pd.read_csv(\"../datasets/pivoted/strutture_sanitarie_pubbliche.csv\", on_bad_lines=\"skip\", delimiter=\";\", encoding=\"utf-8\")\n",
    "df_paraf = pd.read_csv(\"../datasets/cleaned/parafarmacie.csv\", on_bad_lines=\"skip\", delimiter=\";\", encoding=\"utf-8\")\n",
    "df_farma = pd.read_csv(\"../datasets/cleaned/farmacie.csv\", on_bad_lines=\"skip\", delimiter=\";\", encoding=\"utf-8\")\n",
    "\n",
    "df_private.columns = df_private.columns.str.strip()\n",
    "df_pubbliche.columns = df_pubbliche.columns.str.strip()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f75e2680",
   "metadata": {},
   "source": [
    "///some comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "255ab5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from OSMPythonTools.nominatim import Nominatim\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "nominatim = Nominatim()\n",
    "results = []\n",
    "\n",
    "LOC = []\n",
    "LAT = []\n",
    "LON = []\n",
    "\n",
    "for via, citta in zip(df_private[\"Indirizzo\"], df_private[\"Citta\\'\"]):\n",
    "    \n",
    "    via = str(via).lower().replace('a\\'', 'a').replace('e\\'', 'e').replace('i\\'', 'i').replace('o\\'', 'o').replace('u\\'', 'u')\n",
    "    citta = str(citta).lower().replace('a\\'', 'a').replace('e\\'', 'e').replace('i\\'', 'i').replace('o\\'', 'o').replace('u\\'', 'u')\n",
    "\n",
    "    res = nominatim.query(via + \" \" + citta)\n",
    "    results.append(res)\n",
    "\n",
    "for item in results:\n",
    "    tmp = item.toJSON()\n",
    "    if len(tmp) != 0:\n",
    "        LAT.append(tmp[0]['lat'])\n",
    "        LON.append(tmp[0]['lon'])\n",
    "        cap = re.search(r'\\d{5}',tmp[0]['display_name'])\n",
    "        if cap is not None:\n",
    "            LOC.append(cap.group())\n",
    "        else:\n",
    "            LOC.append(0)\n",
    "    else:\n",
    "        LAT.append(0.0)\n",
    "        LON.append(0.0)\n",
    "        LOC.append(0)\n",
    "            \n",
    "df_private.insert(loc=0, column='Longitudine', value=pd.Series(LON))\n",
    "df_private.insert(loc=0, column='Latitudine', value=pd.Series(LAT))\n",
    "df_private.insert(loc=0, column='CAP', value=pd.Series(LOC))\n",
    "\n",
    "filepath = Path('../datasets/completed/private.csv')\n",
    "df_private.to_csv(filepath, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ed39b14",
   "metadata": {},
   "source": [
    "///some comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "640fd973",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_pubbliche' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m LAT \u001b[39m=\u001b[39m []\n\u001b[0;32m     11\u001b[0m LON \u001b[39m=\u001b[39m []\n\u001b[1;32m---> 13\u001b[0m \u001b[39mfor\u001b[39;00m via, cap \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(df_pubbliche[\u001b[39m\"\u001b[39m\u001b[39mIndirizzo\u001b[39m\u001b[39m\"\u001b[39m], df_pubbliche[\u001b[39m\"\u001b[39m\u001b[39mCAP\u001b[39m\u001b[39m\"\u001b[39m]):\n\u001b[0;32m     15\u001b[0m     via \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(via)\u001b[39m.\u001b[39mlower()\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39me\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39me\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39mi\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mi\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39mo\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mo\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39mu\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     16\u001b[0m     cap \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(cap)\u001b[39m.\u001b[39mlower()\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39me\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39me\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39mi\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mi\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39mo\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mo\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39mu\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mu\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_pubbliche' is not defined"
     ]
    }
   ],
   "source": [
    "from OSMPythonTools.nominatim import Nominatim\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "nominatim = Nominatim()\n",
    "results = []\n",
    "\n",
    "COM = []\n",
    "LAT = []\n",
    "LON = []\n",
    "\n",
    "for via, cap in zip(df_pubbliche[\"Indirizzo\"], df_pubbliche[\"CAP\"]):\n",
    "    \n",
    "    via = str(via).lower().replace('a\\'', 'a').replace('e\\'', 'e').replace('i\\'', 'i').replace('o\\'', 'o').replace('u\\'', 'u')\n",
    "    cap = str(cap).lower().replace('a\\'', 'a').replace('e\\'', 'e').replace('i\\'', 'i').replace('o\\'', 'o').replace('u\\'', 'u')\n",
    "\n",
    "    res = nominatim.query(via + \" \" + cap)\n",
    "    results.append(res)\n",
    "    \n",
    "for item in results:\n",
    "    tmp = item.toJSON()\n",
    "    if len(tmp) != 0:\n",
    "        LAT.append(tmp[0]['lat'])\n",
    "        LON.append(tmp[0]['lon'])\n",
    "        com = re.search(r', (\\w*), Sicilia',tmp[0]['display_name'])\n",
    "        if com is not None:\n",
    "            COM.append(com[1])\n",
    "        else:\n",
    "            COM.append(\"\")\n",
    "    else:\n",
    "        LAT.append(0.0)\n",
    "        LON.append(0.0)\n",
    "        COM.append(\"\")\n",
    "            \n",
    "df_pubbliche.insert(loc=0, column='Longitudine', value=pd.Series(LON))\n",
    "df_pubbliche.insert(loc=0, column='Latitudine', value=pd.Series(LAT))\n",
    "df_pubbliche.insert(loc=0, column='Comune', value=pd.Series(COM))\n",
    "\n",
    "filepath = Path('../datasets/completed/pubbliche.csv')\n",
    "df_pubbliche.to_csv(filepath, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e29dcc81",
   "metadata": {},
   "source": [
    "Quest'ultima elaborazione dei dataset √® stata riportata in: `../datasets/completed/`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eab42d63",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Rielaborazione dell'ultima parte del dataset farmacie. Questo perch√® nominatim non √® riuscito a computare alcuni dati. Inserisco il tutto in un csv temporaneo chiamato out.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c78505fe",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nominatim] downloading data: search\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n",
      "[nominatim] downloading data: search\n"
     ]
    }
   ],
   "source": [
    "from OSMPythonTools.nominatim import Nominatim\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"../datasets/Faramacie_temp.csv\", sep=\";\")\n",
    "nominatim = Nominatim()\n",
    "results = []\n",
    "\n",
    "\n",
    "LAT = []\n",
    "LON = []\n",
    "\n",
    "for via, citta in zip(df[\"INDIRIZZO\"], df[\"DESCRIZIONECOMUNE\"]):\n",
    "\n",
    "    via = str(via).lower().replace('a\\'', 'a').replace('e\\'', 'e').replace('i\\'', 'i').replace('o\\'', 'o').replace('u\\'', 'u')\n",
    "    citta = str(citta).lower().replace('a\\'', 'a').replace('e\\'', 'e').replace('i\\'', 'i').replace('o\\'', 'o').replace('u\\'', 'u')\n",
    "\n",
    "    res = nominatim.query(via + \",\" + citta)\n",
    "    results.append(res)\n",
    "\n",
    "for item in results:\n",
    "    tmp = item.toJSON()\n",
    "    if len(tmp) != 0:\n",
    "        LAT.append(tmp[0]['lat'])\n",
    "        LON.append(tmp[0]['lon'])\n",
    "    else:\n",
    "        LAT.append(0.0)\n",
    "        LON.append(0.0)\n",
    "\n",
    "df[\"LONGITUDINE\"] = pd.Series(LON)\n",
    "df[\"LATITUDINE\"] = pd.Series(LAT)\n",
    "#df.insert(loc=0, column='LONGITUDINE', value=pd.Series(LON))\n",
    "#df.insert(loc=0, column='LATITUDINE', value=pd.Series(LAT))\n",
    "\n",
    "filepath = Path('../datasets/out.csv')\n",
    "df.to_csv(filepath, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e1f160aa",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Sistemato il dataset farmacie.csv per ottenere i tipi giusti da pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fb4b854",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "farmacie = pd.read_csv(\"../datasets/completed/farmacie_-_Copy.csv\", sep=\";\")\n",
    "farmacie[\"LATITUDINE\"] = farmacie[\"LATITUDINE\"].apply(lambda x : float(x.replace(',', '.')) if x != '-' else 0.0)\n",
    "farmacie[\"LONGITUDINE\"] = farmacie[\"LONGITUDINE\"].apply(lambda x : float(x.replace(',', '.')) if x != '-' else 0.0)\n",
    "farmacie[\"PARTITAIVA\"] = farmacie[\"PARTITAIVA\"].apply(lambda x : x if x != \"-\" else 0)\n",
    "farmacie[\"INDIRIZZO\"] = farmacie[\"INDIRIZZO\"].apply(lambda x : x.replace('\"', ''))\n",
    "filepath = Path('../datasets/completed/farmacie.csv')\n",
    "farmacie.to_csv(filepath, index=False,sep=\";\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b25305d6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Concat dei due dataset: farmacie e out, ottenuto prima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3145967c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "farmacie = pd.read_csv(\"../datasets/completed/farmacie.csv\", sep=\";\")\n",
    "df = pd.read_csv(\"../datasets/out.csv\")\n",
    "df_finale = pd.concat([farmacie,df], ignore_index=True)\n",
    "filepath = Path('../datasets/out.csv')\n",
    "df_finale.to_csv(filepath, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a8e1e15",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# __4 Trasformazione dei dataset a 5 stelle__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "804fec58",
   "metadata": {},
   "source": [
    "### __4.1 Ontologia__\n",
    "L'ontologia √® stata creata con il software Prot√®g√®..... ontologia_sanita.ttl\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "598e06ae",
   "metadata": {},
   "source": [
    "### __4.2 Creazione grafo RDF__\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "80c8108c",
   "metadata": {},
   "source": [
    "///some comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f7b96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib.parse\n",
    "from rdflib import Graph, Literal, Namespace, URIRef\n",
    "from rdflib.namespace import RDF, RDFS\n",
    "\n",
    "base_uri = \"http://www.sanitasicilia.it/resource/\"\n",
    "g = Graph()\n",
    "\n",
    "sso = Namespace(\"http://www.sanitasicilia.it/ontology/\")\n",
    "g.bind(\"sso\", sso)\n",
    "\n",
    "ssr = Namespace(\"http://www.sanitasicilia.it/resource/\")\n",
    "g.bind(\"ssr\", ssr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5b302e31",
   "metadata": {},
   "source": [
    "///Some comment COMUNI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266baef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def urify(ns, testo):\n",
    "    testo=testo.replace(\" \",\"_\").replace(\"\\'\",\"\")\n",
    "    return ns+urllib.parse.quote(testo)\n",
    "\n",
    "def addTriples(row):\n",
    "    res = URIRef(urify(base_uri, row[0]))\n",
    "    g.add([res, URIRef(RDF.type), URIRef(sso.Comune)])\n",
    "    g.add([res, sso.hasName, Literal(row[0], datatype=XSD.string)])\n",
    "    g.add([res, sso.hasTotalPopulation, Literal(row[1], datatype=XSD.integer)])\n",
    "    g.add([res, sso.hasProvince, Literal(row[2], datatype=XSD.string)])\n",
    "    g.add([res, sso.hasProvinceAcr, Literal(row[3], datatype=XSD.string)])\n",
    "\n",
    "comuni_df = pd.read_csv(\"../datasets/ready/comuni_sicilia.csv\")\n",
    "\n",
    "comuni_df.apply(lambda row : addTriples(row), axis=1)\n",
    "g.serialize(destination='../datasets/rdf/comuni_sicilia.ttl', format='turtle')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5408a0e2",
   "metadata": {},
   "source": [
    "///Some comment FARMACIE E PARAFARMACIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772dae70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addTriples(row):\n",
    "    res = URIRef(urify(base_uri, row[5]))\n",
    "    g.add([res, URIRef(RDF.type), URIRef(sso.Farmacia)])\n",
    "    g.add([res, sso.isIn, URIRef(urify(base_uri, row[2].title()))])\n",
    "    g.add([res, sso.hasCap, Literal(row[3], datatype=XSD.integer) ])\n",
    "    g.add([res, sso.hasAddress, Literal(row[4], datatype=XSD.string) ])\n",
    "    g.add([res, sso.hasName, Literal(row[5], datatype=XSD.string) ])\n",
    "    g.add([res, sso.hasLatitude, Literal(row[6], datatype=XSD.decimal) ])\n",
    "    g.add([res, sso.hasLongitude, Literal(row[7], datatype=XSD.decimal) ])\n",
    "    g.add([res, sso.hasVatNumber, Literal(row[8], datatype=XSD.integer) ])\n",
    "\n",
    "farmacie_df = pd.read_csv(\"../datasets/ready/farmacie.csv\", sep=';')\n",
    "farmacie_df[\"LATITUDINE\"] = farmacie_df[\"LATITUDINE\"].apply(lambda x : float(x.replace(',', '.')) if x != '-' else 0.0)\n",
    "farmacie_df[\"LONGITUDINE\"] = farmacie_df[\"LONGITUDINE\"].apply(lambda x : float(x.replace(',', '.')) if x != '-' else 0.0)\n",
    "farmacie_df[\"PARTITAIVA\"] = farmacie_df[\"PARTITAIVA\"].apply(lambda x : x if x != '-' else 0)\n",
    "\n",
    "parafarmacie_df = pd.read_csv(\"../datasets/ready/parafarmacie.csv\", sep=';')\n",
    "parafarmacie_df[\"LATITUDINE\"] = parafarmacie_df[\"LATITUDINE\"].apply(lambda x : float(x.replace(',', '.')) if x != '-' else 0.0)\n",
    "parafarmacie_df[\"LONGITUDINE\"] = parafarmacie_df[\"LONGITUDINE\"].apply(lambda x : float(x.replace(',', '.')) if x != '-' else 0.0)\n",
    "parafarmacie_df[\"PARTITAIVA\"] = parafarmacie_df[\"PARTITAIVA\"].apply(lambda x : x if x != '-' else 0)\n",
    "\n",
    "g = Graph()\n",
    "farmacie_df.apply(lambda row : addTriples(row), axis=1)\n",
    "g.serialize(destination='../datasets/rdf/farmacie.ttl', format='turtle')\n",
    "\n",
    "g = Graph() #attualmente, per debug rdf separati\n",
    "parafarmacie_df.apply(lambda row : addTriples(row), axis=1)\n",
    "g.serialize(destination='../datasets/rdf/parafarmacie.ttl', format='turtle')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f6aecc33",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "01b41b5b",
   "metadata": {},
   "source": [
    "### __4.3 Interlinking del grafo RDF__\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd77a4b8",
   "metadata": {},
   "source": [
    "///some comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34950e82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0da33a5a",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# __5 Data visualization__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "05cf9238",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# __6 Creazione di un'applicazione__\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e19392e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
